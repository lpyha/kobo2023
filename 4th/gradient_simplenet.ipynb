{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークに対する勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self) -> None:\n",
    "        # 形状2x3の重みパラメータ\n",
    "        self.weight = np.random.randn(2, 3)   # ガウス分布で初期化\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.weight)\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        c = np.max(a)\n",
    "        exp_a = np.exp(a - c)   # オーバーフロー対策\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        return exp_a / sum_exp_a\n",
    "    \n",
    "    def cross_entropy_error(self, y, t):\n",
    "        '''\n",
    "        引数y, tはnumpy配列とする。\n",
    "        np.logの計算時、微小な値であるdeltaを足す\n",
    "        np.log(0)の場合-infとなるのでそれを防止する\n",
    "        '''\n",
    "        delta = 1e-7\n",
    "        return -np.sum(t * np.log(y+ delta))\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        '''\n",
    "        損失関数を求める\n",
    "        x: 入力データ\n",
    "        t: 正解ラベル\n",
    "        '''\n",
    "        z = self.predict(x)\n",
    "        y = self.softmax(z)\n",
    "        loss = self.cross_entropy_error(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def numerical_gradient(self, f, x):\n",
    "        '''\n",
    "        勾配を求める\n",
    "        '''\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x) # xと同じ形状の配列を生成\n",
    "        \n",
    "        for idx in range(x.size):\n",
    "            tmp_val = x[idx]\n",
    "            # f(x+h)の計算\n",
    "            x[idx] = tmp_val + h\n",
    "            fxh1 = f(x)\n",
    "            \n",
    "            # f(x-h)の計算\n",
    "            x[idx] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "            \n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            x[idx] = tmp_val\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38998098 -2.04718829  0.0664549 ]\n",
      " [ 0.02466794 -0.21165986 -1.36151646]]\n",
      "1.4905725711256286\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m ret \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mloss(x, t)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(ret)\n\u001b[1;32m---> 13\u001b[0m dw \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mnumerical_gradient(ret, net\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(dw)\n",
      "Cell \u001b[1;32mIn[24], line 49\u001b[0m, in \u001b[0;36msimpleNet.numerical_gradient\u001b[1;34m(self, f, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# f(x+h)の計算\u001b[39;00m\n\u001b[0;32m     48\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m+\u001b[39m h\n\u001b[1;32m---> 49\u001b[0m fxh1 \u001b[39m=\u001b[39m f(x)\n\u001b[0;32m     51\u001b[0m \u001b[39m# f(x-h)の計算\u001b[39;00m\n\u001b[0;32m     52\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m-\u001b[39m h\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# simpleNetのサンプル\n",
    "net = simpleNet()\n",
    "print(net.weight)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "\n",
    "np.argmax(p)    # 最大値のインデックス\n",
    "t = np.array([0,0,1])   # 正解ラベル\n",
    "ret = net.loss(x, t)\n",
    "print(ret)\n",
    "\n",
    "dw = net.numerical_gradient(ret, net.weight)\n",
    "print(dw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

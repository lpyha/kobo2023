{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid関数の実装\n",
    "$h(x) = \\frac{1}{1 + \\exp(-x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tmp_sys\\ipykernel_1440\\3095983265.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(x>0, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "def step_func(x):\n",
    "    '''step関数、入力が0を超えたら1を出力、それ以外は0を出力'''\n",
    "    return np.array(x>0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_func(x)\n",
    "\n",
    "#plt.plot(x, y)\n",
    "#plt.ylim(-0.1, 1.1)\n",
    "#plt.title(\"step_func\")\n",
    "#plt.show()\n",
    "\n",
    "def sigmoid_func(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "y = sigmoid_func(x)\n",
    "\n",
    "#plt.plot(x, y)\n",
    "#plt.ylim(-0.1, 1.1)\n",
    "#plt.title(\"sigmoid_func\")\n",
    "#plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU(Rectified Linear Unit)関数\n",
    "入力が0を超えていれば、その入力をそのまま出力し、0以下ならば0を出力する関数\n",
    "\n",
    "$h(x)=\\begin{cases}\n",
    "        x & (x > 0)\\\\\n",
    "        0 & (x \\le 0)\n",
    "        \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu_func(x)\n",
    "\n",
    "#plt.plot(x, y)\n",
    "#plt.xlim(-5.1, 5.1)\n",
    "#plt.ylim(-0.1, 5.1)\n",
    "#plt.title(\"relu_func\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31682708 0.69627909]]\n"
     ]
    }
   ],
   "source": [
    "def identify_func(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([[0.1, 0.2, 0.3]])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([[0.1, 0.2]])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "    \n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid_func(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid_func(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identify_func(a3)\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層の設計\n",
    "ニューラルネットワークでは、分類問題と回帰問題の両方を用いることができる。どちらかを用いるかで、出力層の活性化関数を変更する必要がある。\\\n",
    "一般に回帰問題には恒等関数、分類問題にはソフトマックス関数を使う\\\n",
    "分類問題->データがどのクラスに属するか\\\n",
    "回帰問題->入力データから数値の予測"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恒等関数とソフトマックス関数\n",
    "恒等関数は入力をそのまま出力する\\\n",
    "ソフトマックス関数は次の式で示される\\\n",
    "$y_{k} = \\frac{\\exp(a_{k})}{\\sum\\limits_{i=0}^{n} \\exp(a_{i})}$\\\n",
    "オーバーフローを意識して作らなければいけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_func(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)   # オーバーフロー対策\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソフトマックス関数の特徴\n",
    "ソフトマックス関数の出力は0から1.0の間の実数になり、出力の総和は1になる。この性質からソフトマックス関数の出力を「確率」として解釈することができる。\\\n",
    "ニューラルネットワークのクラス分類では一般的に出力の一番大きいニューロンに相当するクラスだけを認識結果とする。そして出力の一番大きいニューロンの場所は変わらない。そのためニューラルネットワークが分類を行う際には、出力層のソフトマックス関数を省略することができる。また指数関数の計算はそれなりにコンピューターの計算が必要になるので、出力層のソフトマックス関数は省略するのが一般的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
